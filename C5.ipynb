{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3dce150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7f8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d44fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f_name = (train_df[\"filename\"].values).astype(str)\n",
    "y_train = train_df.loc[:,\"target_00\":].values\n",
    "X_test_f_name = (test_df[\"filename\"].values).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b8b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array([])\n",
    "for i in range(len(X_train_f_name)):\n",
    "    file_name=(7-len(X_train_f_name[i]))*'0'+X_train_f_name[i]\n",
    "    f = open('D:/data/train/'+file_name, 'r')\n",
    "    X_train=np.append(X_train, f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d5d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array([])\n",
    "for i in range(len(X_test_f_name)):\n",
    "    file_name=(7-len(X_test_f_name[i]))*'0'+X_test_f_name[i]\n",
    "    f = open('D:/data/test/'+file_name, 'r')\n",
    "    X_test=np.append(X_test, f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4db058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    #transform in lowercase\n",
    "    text=text.lower()\n",
    "    #suppress whitespaces in the begining and the end \n",
    "    text=re.sub(r\"^\\s+|\\s+$\", \"\",text )\n",
    "    #supress caracters that are neither alphabet neither whitespace\n",
    "    text=re.sub('[^A-Za-z ]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70028685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698ec343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    tokens = nltk.word_tokenize (text)\n",
    "    text =' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71cae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f62005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_stopwords(text, is_lower_case=False):\n",
    "    tokens = nltk.word_tokenize (text)\n",
    "    #tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ca3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(liste):\n",
    "    \n",
    "    processed_text=[]\n",
    "    for  i in range (len(liste)):\n",
    "        text=liste[i]\n",
    "        \n",
    "        #supprimer les chiffres et supprimer les carctères qui ne sont ni des alphabets ni des espaces ,et les espaces au début et à la fin ,transformer en minuscule,\n",
    "        text=clean_text(text)\n",
    "            \n",
    "        # supprimer extra newlines\n",
    "        text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',text)\n",
    "        \n",
    "        # raciner le texte\n",
    "        text= lemmatize_text(text)\n",
    "    \n",
    "        # supprimer extra espaces\n",
    "        text= re.sub(' +', ' ', text)\n",
    "        \n",
    "        # supprimer les mots vides\n",
    "        text = strip_stopwords(text, is_lower_case=True)\n",
    "            \n",
    "        processed_text.append(text)\n",
    "        \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd04834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93eecd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=list(X_train)\n",
    "corpus=cleaning(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30bd4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lT=list(X_test)\n",
    "corpusT=cleaning(lT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9cd65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "XT=vectorizer.fit_transform(corpusT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc40a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98c10cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(solver='liblinear',random_state=0)\n",
    "sgd=SGDClassifier()\n",
    "svc=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "629d96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab6ee2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e69ba07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  SVC\n",
      "hamming loss : 0.05856469355683604\n"
     ]
    }
   ],
   "source": [
    "clf=OneVsRestClassifier(svc)\n",
    "clf.fit(X_tr,np.array(y_tr))\n",
    "y_predict=clf.predict(X_te)\n",
    "hamming_accuracy=hamming_loss(y_te,y_predict)\n",
    "print(\"Clf: \",svc.__class__.__name__)\n",
    "print(\"hamming loss :\",hamming_accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50e02352",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=clf.predict(XT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d229258",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['target_00', 'target_01', 'target_02', 'target_03',\n",
    "       'target_04', 'target_05', 'target_06', 'target_07', 'target_08',\n",
    "       'target_09', 'target_10', 'target_11', 'target_12', 'target_13',\n",
    "       'target_14', 'target_15', 'target_16', 'target_17', 'target_18',\n",
    "       'target_19', 'target_20', 'target_21', 'target_22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "727e3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[columns]=predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f9e7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"sub5.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
